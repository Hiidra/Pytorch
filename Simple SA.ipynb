{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True # 用以保证实验的可重复性，运行的结果完全一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT=data.Field() #  define how  data should be processed\n",
    "LABEL=data.LabelField(dtype=torch.torch.float) # a special subset of the Field class specifically used for handling labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\imdb\\aclImdb_v1.tar.gz: 100%|███████████████████████████████████████████████| 84.1M/84.1M [00:30<00:00, 2.80MB/s]\n"
     ]
    }
   ],
   "source": [
    "# 以下代码自动下载IMDb数据集并将其拆分为torchtext.datasets对象的规范训练/测试拆分\n",
    "train_data,test_data=datasets.IMDB.splits(TEXT,LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train examples:25000\n",
      "number of train examples:25000\n"
     ]
    }
   ],
   "source": [
    "# print(f'{xxxx}')   将{xxxx}中的表达式执行并输出\n",
    "print(f'number of train examples:{len(train_data)}')\n",
    "print(f'number of train examples:{len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Bromwell', 'High', 'is', 'a', 'cartoon', 'comedy.', 'It', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life,', 'such', 'as', '\"Teachers\".', 'My', '35', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'Bromwell', \"High's\", 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', '\"Teachers\".', 'The', 'scramble', 'to', 'survive', 'financially,', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', \"teachers'\", 'pomp,', 'the', 'pettiness', 'of', 'the', 'whole', 'situation,', 'all', 'remind', 'me', 'of', 'the', 'schools', 'I', 'knew', 'and', 'their', 'students.', 'When', 'I', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school,', 'I', 'immediately', 'recalled', '.........', 'at', '..........', 'High.', 'A', 'classic', 'line:', 'INSPECTOR:', \"I'm\", 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers.', 'STUDENT:', 'Welcome', 'to', 'Bromwell', 'High.', 'I', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'Bromwell', 'High', 'is', 'far', 'fetched.', 'What', 'a', 'pity', 'that', 'it', \"isn't!\"], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# pass random seed to the random_state argument, \n",
    "# ensuring that get the same train/validation split each time\n",
    "# default this splits 70/30\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train_data:17500\n",
      "Number of valid_data:7500\n",
      "Number of test_data:25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of train_data:{len(train_data)}')\n",
    "print(f'Number of valid_data:{len(valid_data)}')\n",
    "print(f'Number of test_data:{len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following builds the vocabulary, only keeping the most common max_size tokens.\n",
    "MAX_VOCAB_SIZE = 25000\n",
    "TEXT.build_vocab(train_data,max_size=MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25002\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "# 25002,One of the addition tokens is the <unk> token and the other is a <pad> token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 201637), ('a', 108267), ('and', 106782), ('of', 99982), ('to', 92808), ('is', 72305), ('in', 59801), ('I', 45880), ('that', 45107), ('this', 40094), ('it', 38059), ('/><br', 35752), ('was', 32706), ('as', 29668), ('with', 29022), ('for', 28896), ('The', 23668), ('but', 23604), ('on', 21550), ('movie', 21368), ('are', 20095), ('his', 19270), ('film', 19179), ('have', 19018), ('not', 18433), ('be', 17802), ('you', 17724), ('he', 15115), ('by', 15037), ('at', 14936), ('one', 14485), ('an', 14322), ('from', 13365), ('who', 13143), ('like', 12805), ('all', 12603), ('they', 12476), ('so', 11459), ('has', 11440), ('just', 11439), ('or', 11404), ('about', 11370), ('her', 11077), ('out', 10008), ('some', 9944), ('very', 9237), ('more', 9011), ('This', 8606), ('would', 8247), ('what', 8221)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(TEXT.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator,valid_iterator,test_iterator=data.BucketIterator.splits(\n",
    "    (train_data,valid_data,test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_dim,embedding_dim,hidden_dim,output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding=nn.Embedding(input_dim,embedding_dim)\n",
    "        self.rnn=nn.RNN(embedding_dim,hidden_dim)\n",
    "        self.fc=nn.Linear(hidden_dim,output_dim)\n",
    "    def forward(self,text):\n",
    "        # text = [sent len, batch size]\n",
    "        embedded=self.embedding(text)\n",
    "        # embedded = [sent len, batch size, emb dim]\n",
    "        output,hidden=self.rnn(embedded)\n",
    "        # output = [sent len, batch size, hid dim]\n",
    "        # hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        # 为了得到hidden=[batch size, hid dim],需要squeeze一下，移除纬度为1的维度\n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM=len(TEXT.vocab)\n",
    "EMBEDDING_DIM=100\n",
    "HIDDEN_DIM=256\n",
    "OUTPUT_DIM=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RNN(INPUT_DIM,EMBEDDING_DIM,HIDDEN_DIM,OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,592,105 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight   Parameter containing:\n",
      "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.5903, -0.1947, -0.2415],\n",
      "        [ 1.3204,  1.5997, -1.0792,  ...,  0.6060,  0.2209, -0.8245],\n",
      "        [ 0.7289, -0.7336,  1.5624,  ..., -0.5592, -0.4480, -0.6476],\n",
      "        ...,\n",
      "        [ 0.0914,  1.5196,  0.4670,  ...,  0.6393, -0.0332,  0.0185],\n",
      "        [-0.6290,  0.4650, -0.7165,  ..., -1.3171,  2.0381, -2.0497],\n",
      "        [-1.1222, -0.0240, -1.0878,  ..., -0.4948, -0.3874,  0.0339]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_ih_l0   Parameter containing:\n",
      "tensor([[ 0.0484, -0.0203,  0.0480,  ..., -0.0512, -0.0010, -0.0363],\n",
      "        [ 0.0486,  0.0025, -0.0124,  ...,  0.0535,  0.0616,  0.0293],\n",
      "        [ 0.0418, -0.0329,  0.0084,  ...,  0.0476, -0.0291,  0.0144],\n",
      "        ...,\n",
      "        [-0.0199,  0.0285, -0.0489,  ..., -0.0426,  0.0226, -0.0279],\n",
      "        [ 0.0444,  0.0087,  0.0575,  ..., -0.0464, -0.0326,  0.0446],\n",
      "        [ 0.0614, -0.0383,  0.0144,  ..., -0.0026, -0.0577, -0.0197]],\n",
      "       requires_grad=True)\n",
      "rnn.weight_hh_l0   Parameter containing:\n",
      "tensor([[ 0.0429, -0.0497,  0.0465,  ..., -0.0140,  0.0554, -0.0281],\n",
      "        [-0.0100, -0.0179,  0.0129,  ...,  0.0270,  0.0339,  0.0208],\n",
      "        [-0.0443, -0.0229, -0.0614,  ..., -0.0181, -0.0328, -0.0493],\n",
      "        ...,\n",
      "        [ 0.0237,  0.0555,  0.0408,  ..., -0.0225, -0.0338, -0.0467],\n",
      "        [-0.0079, -0.0077,  0.0305,  ..., -0.0425,  0.0531,  0.0416],\n",
      "        [-0.0431,  0.0232,  0.0152,  ..., -0.0074, -0.0117,  0.0191]],\n",
      "       requires_grad=True)\n",
      "rnn.bias_ih_l0   Parameter containing:\n",
      "tensor([ 1.8789e-02, -3.2913e-02,  5.4046e-02,  1.1215e-02, -1.6592e-02,\n",
      "        -5.9410e-02,  1.3773e-02, -3.7467e-02,  1.6023e-02,  4.3422e-02,\n",
      "        -1.4563e-02,  5.6511e-02,  6.2642e-03, -1.5839e-02,  5.5968e-02,\n",
      "        -1.9909e-02, -2.2919e-02, -3.6973e-02, -5.9339e-02,  7.7139e-03,\n",
      "         1.0052e-02, -3.6551e-02,  5.2303e-02, -4.3346e-02,  4.5805e-02,\n",
      "        -5.7762e-02,  1.3457e-02, -1.5781e-02, -1.9001e-02,  3.5357e-02,\n",
      "        -4.8912e-02,  4.4137e-02,  3.7997e-02,  3.2476e-02,  6.0541e-03,\n",
      "        -4.6944e-02,  3.5186e-03, -1.0676e-02, -2.6128e-02,  5.7206e-02,\n",
      "         4.1225e-02, -3.0166e-02, -4.9413e-02,  2.9726e-02, -1.3146e-02,\n",
      "         2.1225e-02, -2.0520e-02,  5.9967e-02, -1.4810e-02,  4.7995e-02,\n",
      "        -2.4305e-03, -2.8684e-03, -5.3235e-02,  3.1330e-02,  2.3712e-02,\n",
      "         1.4307e-02, -3.1319e-02, -5.1632e-02, -5.7203e-02, -1.1648e-02,\n",
      "         1.9732e-02,  4.2688e-02, -1.8094e-02,  5.4463e-02, -2.1333e-02,\n",
      "        -4.0323e-02,  1.4849e-02,  7.6102e-03, -2.4658e-02, -6.1570e-02,\n",
      "         5.2047e-03,  3.5586e-02, -2.2978e-02,  4.5677e-02,  9.8402e-03,\n",
      "        -2.9035e-02,  5.1551e-05,  1.4623e-02, -1.1850e-02,  6.0684e-02,\n",
      "         4.5486e-02, -2.8170e-02, -1.4034e-02,  6.0360e-02, -5.7756e-02,\n",
      "        -4.8075e-03, -4.1478e-02, -6.1209e-02,  1.4524e-02,  5.7920e-03,\n",
      "        -3.9700e-02,  2.8181e-02, -6.2541e-03, -2.1595e-02, -6.0309e-02,\n",
      "        -2.5631e-02, -1.8598e-02,  5.8865e-02,  2.3869e-02, -3.8985e-03,\n",
      "         7.1074e-03, -4.3879e-02,  1.4110e-02, -1.5257e-02, -2.9437e-02,\n",
      "        -5.0306e-02,  3.2506e-02,  1.2306e-02,  5.3663e-03,  1.1446e-02,\n",
      "        -4.1753e-03, -5.7555e-02,  6.0424e-02,  5.8802e-02, -1.2119e-02,\n",
      "        -1.8489e-02, -3.1919e-02, -1.7538e-02,  3.3869e-03,  6.1611e-02,\n",
      "        -2.8105e-03, -4.7080e-02, -2.6462e-02, -6.1647e-02, -1.3916e-02,\n",
      "         4.0059e-02, -2.4060e-02,  4.3172e-02,  1.2631e-02, -4.6448e-02,\n",
      "         2.5378e-02, -2.6271e-03,  4.1044e-02,  3.7875e-02,  1.0496e-02,\n",
      "        -5.4345e-02,  5.5537e-02,  3.7900e-02,  3.7525e-02,  6.0519e-02,\n",
      "        -2.4503e-02,  8.1552e-03,  3.6646e-02,  4.2451e-02, -2.1550e-02,\n",
      "        -5.8322e-02, -2.5021e-02,  4.6712e-03,  1.4824e-02, -5.2719e-03,\n",
      "        -2.8919e-02,  5.3967e-02,  4.1202e-02, -5.1878e-02,  2.9613e-02,\n",
      "        -6.1749e-02, -3.5344e-02, -3.1486e-02, -3.0101e-02, -5.4058e-02,\n",
      "        -3.3689e-02,  3.6686e-02,  7.6651e-03,  2.3560e-02, -1.9203e-02,\n",
      "        -5.8347e-02, -3.1122e-02,  3.4260e-02,  1.8087e-02,  2.7107e-02,\n",
      "        -2.2864e-02, -2.8230e-04,  1.2944e-02,  4.9142e-02, -1.9590e-02,\n",
      "        -5.5829e-02, -4.6675e-02, -4.1777e-02,  2.2638e-02,  2.2195e-02,\n",
      "        -8.4196e-03,  2.1661e-02, -2.9886e-02, -6.2776e-03,  1.3659e-02,\n",
      "        -3.8523e-02,  4.2576e-03, -6.6685e-03, -5.5717e-02, -2.8992e-02,\n",
      "        -1.8446e-02, -1.2721e-02,  2.8823e-02,  3.8524e-02, -2.5943e-02,\n",
      "         4.4176e-02, -5.0263e-02,  2.3650e-02,  4.0393e-02, -3.4220e-02,\n",
      "        -4.1614e-02, -5.6058e-02,  2.8549e-02,  1.4835e-02, -3.4803e-02,\n",
      "         2.0758e-02, -5.0638e-02,  4.2316e-02, -5.0646e-02,  4.0154e-02,\n",
      "        -3.8806e-02,  6.1322e-02,  4.5722e-02, -2.7874e-02, -6.0086e-02,\n",
      "        -2.8397e-02, -4.9270e-02, -3.1190e-02, -4.3550e-02,  3.2643e-02,\n",
      "         3.7021e-02, -1.5987e-02, -2.1622e-02,  5.1505e-02, -3.4132e-02,\n",
      "         4.0308e-02, -2.0501e-02, -4.1436e-02,  6.0095e-03, -1.9734e-02,\n",
      "         3.5165e-02,  4.7922e-02, -3.1505e-02,  4.1938e-02, -2.7419e-03,\n",
      "         3.8432e-02, -5.6689e-02,  3.8366e-02,  4.2080e-02, -4.8144e-02,\n",
      "         4.6955e-02,  2.8831e-02,  6.1010e-02, -4.1967e-02,  4.1319e-02,\n",
      "        -9.2817e-03,  4.8626e-02, -1.0369e-02,  1.9796e-02,  4.4090e-02,\n",
      "         5.4890e-02, -5.4942e-03, -3.8742e-02, -6.1701e-02, -4.1478e-02,\n",
      "        -5.4746e-02], requires_grad=True)\n",
      "rnn.bias_hh_l0   Parameter containing:\n",
      "tensor([ 0.0469,  0.0183, -0.0534,  0.0093, -0.0424, -0.0092, -0.0449, -0.0143,\n",
      "         0.0225,  0.0016, -0.0088,  0.0090, -0.0365, -0.0039,  0.0049, -0.0103,\n",
      "        -0.0501,  0.0192,  0.0139,  0.0400, -0.0350, -0.0252, -0.0168,  0.0328,\n",
      "        -0.0416,  0.0450, -0.0027, -0.0032, -0.0171, -0.0152, -0.0319, -0.0438,\n",
      "        -0.0389, -0.0367, -0.0343, -0.0288, -0.0599, -0.0347, -0.0130,  0.0127,\n",
      "         0.0123, -0.0146,  0.0455, -0.0605,  0.0312, -0.0316, -0.0297, -0.0221,\n",
      "         0.0474,  0.0301,  0.0101, -0.0400, -0.0217, -0.0168,  0.0410, -0.0612,\n",
      "         0.0528,  0.0040,  0.0168,  0.0289,  0.0270,  0.0481,  0.0030, -0.0110,\n",
      "        -0.0491, -0.0145,  0.0475,  0.0399,  0.0047,  0.0178,  0.0481,  0.0247,\n",
      "        -0.0228,  0.0389, -0.0622,  0.0288,  0.0048, -0.0320, -0.0293, -0.0210,\n",
      "        -0.0605, -0.0358,  0.0570,  0.0011, -0.0462, -0.0202,  0.0508,  0.0243,\n",
      "         0.0096, -0.0245, -0.0343, -0.0074, -0.0358, -0.0526,  0.0009, -0.0227,\n",
      "         0.0094,  0.0582, -0.0404, -0.0291,  0.0604, -0.0259, -0.0525, -0.0248,\n",
      "        -0.0069,  0.0050, -0.0512,  0.0108,  0.0543,  0.0421,  0.0276, -0.0127,\n",
      "         0.0408,  0.0029, -0.0243,  0.0419,  0.0419, -0.0441,  0.0259, -0.0572,\n",
      "         0.0122, -0.0592, -0.0025, -0.0059,  0.0153,  0.0426,  0.0447, -0.0565,\n",
      "         0.0196,  0.0243, -0.0447, -0.0013,  0.0418,  0.0450, -0.0016, -0.0041,\n",
      "         0.0009,  0.0450,  0.0264,  0.0014, -0.0134,  0.0258, -0.0137,  0.0596,\n",
      "         0.0564, -0.0147,  0.0460,  0.0360, -0.0358,  0.0457, -0.0404,  0.0419,\n",
      "         0.0126, -0.0440, -0.0242, -0.0600,  0.0329, -0.0070, -0.0024,  0.0293,\n",
      "         0.0396,  0.0128, -0.0399, -0.0050, -0.0004, -0.0140, -0.0611, -0.0494,\n",
      "         0.0395, -0.0024, -0.0320, -0.0363, -0.0088,  0.0108,  0.0560, -0.0450,\n",
      "         0.0035,  0.0506,  0.0268, -0.0425,  0.0446, -0.0471, -0.0133,  0.0097,\n",
      "        -0.0357,  0.0270, -0.0183,  0.0333,  0.0401,  0.0303, -0.0222,  0.0295,\n",
      "        -0.0276,  0.0481, -0.0330,  0.0434, -0.0323, -0.0302,  0.0058,  0.0521,\n",
      "        -0.0003, -0.0379, -0.0624, -0.0548, -0.0507, -0.0615,  0.0428,  0.0165,\n",
      "        -0.0354,  0.0421, -0.0325, -0.0282, -0.0166,  0.0524,  0.0193, -0.0046,\n",
      "        -0.0097, -0.0422, -0.0131, -0.0209, -0.0145, -0.0334, -0.0265,  0.0086,\n",
      "         0.0528,  0.0317,  0.0394,  0.0473, -0.0498,  0.0196,  0.0092, -0.0022,\n",
      "        -0.0489,  0.0577, -0.0034,  0.0455,  0.0049, -0.0379, -0.0055,  0.0157,\n",
      "         0.0396,  0.0512,  0.0578, -0.0330,  0.0041, -0.0387,  0.0554,  0.0417,\n",
      "        -0.0508, -0.0030, -0.0547, -0.0466, -0.0070,  0.0201,  0.0502, -0.0316],\n",
      "       requires_grad=True)\n",
      "fc.weight   Parameter containing:\n",
      "tensor([[-0.0444,  0.0338, -0.0363,  0.0213, -0.0189,  0.0303,  0.0241,  0.0017,\n",
      "         -0.0334, -0.0237, -0.0603,  0.0425,  0.0285,  0.0104,  0.0205, -0.0498,\n",
      "         -0.0032,  0.0360, -0.0556, -0.0391, -0.0582,  0.0282, -0.0265, -0.0026,\n",
      "         -0.0321,  0.0453, -0.0606, -0.0368,  0.0493, -0.0547, -0.0527, -0.0431,\n",
      "          0.0454,  0.0249,  0.0557, -0.0450, -0.0578,  0.0020,  0.0175, -0.0068,\n",
      "         -0.0265,  0.0348, -0.0402,  0.0068, -0.0539,  0.0086, -0.0288,  0.0022,\n",
      "         -0.0553,  0.0439, -0.0384,  0.0412,  0.0496,  0.0541,  0.0174, -0.0093,\n",
      "          0.0246,  0.0031, -0.0147,  0.0047,  0.0496,  0.0174, -0.0151, -0.0332,\n",
      "          0.0412, -0.0419,  0.0613, -0.0162, -0.0457,  0.0190,  0.0509,  0.0361,\n",
      "         -0.0615,  0.0507, -0.0310, -0.0360,  0.0219,  0.0615,  0.0060,  0.0145,\n",
      "          0.0083, -0.0121, -0.0386, -0.0172, -0.0353, -0.0401,  0.0318, -0.0603,\n",
      "          0.0174,  0.0420, -0.0090, -0.0073, -0.0252,  0.0456, -0.0068,  0.0564,\n",
      "         -0.0289,  0.0437, -0.0232,  0.0474,  0.0613,  0.0389, -0.0492, -0.0427,\n",
      "          0.0174, -0.0567, -0.0187,  0.0517,  0.0327,  0.0320, -0.0385, -0.0050,\n",
      "         -0.0476,  0.0166, -0.0224, -0.0041,  0.0165,  0.0565, -0.0027, -0.0370,\n",
      "          0.0158, -0.0607, -0.0440, -0.0011,  0.0073,  0.0247,  0.0499, -0.0113,\n",
      "         -0.0208, -0.0467,  0.0521,  0.0596, -0.0088, -0.0118, -0.0019, -0.0105,\n",
      "          0.0013, -0.0441, -0.0293,  0.0352,  0.0017,  0.0329, -0.0010,  0.0451,\n",
      "          0.0179,  0.0186, -0.0021, -0.0359, -0.0312,  0.0449,  0.0066,  0.0449,\n",
      "         -0.0054, -0.0523, -0.0618,  0.0048, -0.0376,  0.0455, -0.0419, -0.0007,\n",
      "         -0.0086,  0.0380,  0.0348,  0.0408,  0.0105, -0.0623,  0.0355, -0.0328,\n",
      "         -0.0026,  0.0340,  0.0114, -0.0110, -0.0295, -0.0362,  0.0140,  0.0346,\n",
      "          0.0191, -0.0044,  0.0417, -0.0285, -0.0015, -0.0562, -0.0250, -0.0393,\n",
      "         -0.0362,  0.0569,  0.0416,  0.0433,  0.0239, -0.0179, -0.0169,  0.0509,\n",
      "          0.0521,  0.0104, -0.0489, -0.0353,  0.0155, -0.0275,  0.0349,  0.0182,\n",
      "         -0.0523,  0.0298,  0.0415,  0.0577, -0.0014, -0.0164, -0.0239, -0.0355,\n",
      "         -0.0361,  0.0347, -0.0186, -0.0396, -0.0181,  0.0458, -0.0287, -0.0299,\n",
      "         -0.0290,  0.0179,  0.0529,  0.0575,  0.0084, -0.0427, -0.0252, -0.0148,\n",
      "         -0.0128,  0.0082,  0.0588,  0.0514, -0.0159,  0.0377,  0.0599,  0.0606,\n",
      "         -0.0099,  0.0314,  0.0339,  0.0159, -0.0457, -0.0519, -0.0549, -0.0364,\n",
      "          0.0075, -0.0068,  0.0196,  0.0573, -0.0473, -0.0267,  0.0090,  0.0441,\n",
      "         -0.0434,  0.0589,  0.0035, -0.0411,  0.0501, -0.0478,  0.0603, -0.0110]],\n",
      "       requires_grad=True)\n",
      "fc.bias   Parameter containing:\n",
      "tensor([-0.0620], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 可学习的参数通过net.parameters()返回，net.named_parameters可同时返回科学系的参数及名称\n",
    "for name,p in model.named_parameters():\n",
    "    print(name,' ',p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer=optim.SGD(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCEWithLogitsLoss carries out both the sigmoid and the binary cross entropy steps.\n",
    "criterion=nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(device)\n",
    "criterion=criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds,y):\n",
    "    rounded_pred=torch.round(torch.sigmoid(preds))\n",
    "    correct=(rounded_pred==y).float()\n",
    "    acc=correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,iterator,optimizer,critetion):\n",
    "    epoch_loss=0\n",
    "    epoch_acc=0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions=model(batch.text).squeeze(1)\n",
    "        loss=criterion(predictions,batch.label)\n",
    "        acc=accuracy(predictions,batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss+=loss.item()\n",
    "        epoch_acc+=acc.item()\n",
    "    return epoch_loss/len(iterator),epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,iterator,critetion):\n",
    "    epoch_loss=0\n",
    "    epoch_acc=0\n",
    "    model.eval()\n",
    "    for batch in iterator:\n",
    "        predictions=model(batch.text).squeeze(1)\n",
    "        loss=criterion(predictions,batch.label)\n",
    "        acc=accuracy(predictions,batch.label)\n",
    "        epoch_loss+=loss.item()\n",
    "        epoch_acc+=acc.item()\n",
    "    return epoch_loss/len(iterator),epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.694 | Train Acc: 49.67%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 48.95%\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.63%\n",
      "\t Val. Loss: 0.698 |  Val. Acc: 51.03%\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.26%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 48.61%\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.02%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 48.78%\n",
      "\tTrain Loss: 0.693 | Train Acc: 49.90%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 48.83%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "N_EPOCHS=5\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss,train_acc=train(model,train_iterator,optimizer,criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
